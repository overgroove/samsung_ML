{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree(CART : Classification and Regression Tree)\n",
    "> **`Decision Tree`** 모델은 **예측/분류가 모두 가능**한 **지도학습** 머신러닝 모델이다.   \n",
    "스무고개 게임을 하듯 여러 개의 가정을 데이터에 반영하고 이를 바탕으로 결정경계(decision boundary)를 생성  \n",
    "모델 예측 및 분류 결과에 따른 해석이 굉장히 용이하여 **모델 해석이 필요한 문제에 사용**한다.ex)신용평가, 모델분류  \n",
    "최근에는 `Decision Tree`모델을 베이스로 한 부스팅 트리 모델(**`Xgboost`**, **`LightGBM`**, **`Catboost`**)등으로 데이터분석 대회 수상을 하면서 실무 적용 케이스가 많아졌다.\n",
    "\n",
    "### 모델구조\n",
    "> 뿌리 노드(root node) : 최상위 노드, 모든 샘플 포함  \n",
    "잎 노드(leaf node) : 최하위 노드, 여기에 속한 샘플이 어떤 클래스인지 결정 됨  \n",
    "노드(node) : 뿌리 노드와 잎 노드 사이에 있는 노드  \n",
    "가지(branch) : 노드를 나누는 기준  \n",
    "깊이(depth) : 뿌리 노드와 잎 노드 까지의 노드 갯수\n",
    "\n",
    "<img src=\"./image/27.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델학습\n",
    "#### 불순도\n",
    "> `Decision Tree` 모델을 학습시키는 방법  \n",
    "정보화 이론에서 사용하는 Gini 계수와 엔트로피를 사용한다.  \n",
    "불순도가 0.5에 가까수록 불순도가 높고 0 혹은 1에 가까울 수록 순도가 높다.  \n",
    "즉, 한 노드의 불순도가 가능한 많이 떨어지도록(순도가 올라가도록) 노드를 나눈다.\n",
    "\n",
    "$$ Gini = 1 - \\sum_1^n{(p_i)^2} $$\n",
    "\n",
    "$$ Entropy = - \\sum_1^n{p_iln(p_i)} $$\n",
    "\n",
    "#### Gini index\n",
    "위 예시에서 뿌리 노드 기준 지니계수 계산법  \n",
    "class1 : 삼각형  \n",
    "class2 : 동그라미  \n",
    ">X < 0\n",
    ">> True = class1 3개, class2 4개  \n",
    "$1 - ({3 \\over 3+4})^2 - ({4 \\over 3+4})^2 = 0.48$  \n",
    "False = class1 4개, class2 3개  \n",
    "$1 - ({4 \\over 4+3})^2 - ({3 \\over 4+3})^2 = 0.48$  \n",
    "total Gini 계수  \n",
    "$1 - ({7 \\over 7+7})0.48 - ({7 \\over 7+7})0.48 = 0.52$\n",
    "\n",
    "위 예시에서 잎 노드 기준 지니계수 계산법  \n",
    "class1 : 삼각형  \n",
    "class2 : 동그라미  \n",
    ">Y < 1\n",
    ">> True = class1 3개, class2 0개  \n",
    "$1 - ({3 \\over 3})^2 - ({0 \\over 3})^2 = 0$  \n",
    "False = class1 0개, class2 4개  \n",
    "$1 - ({0 \\over 4})^2 - ({4 \\over 4})^2 = 0$  \n",
    "total Gini 계수  \n",
    "$1 - ({3 \\over 3+4})0 - ({4 \\over 3+4})0 = 1$\n",
    "\n",
    "위의 예시에서 계산한 total Gini 계수가 곧 Decision tree 모델의 비용함수가 된다.  \n",
    "이를 바탕으로 더 나은 선택을 하게 되는 결정경계를 생성하는 방법으로 데이터를 학습하는데 이를 greedy 알고리즘이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree classifier 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.690669Z",
     "start_time": "2023-03-30T08:17:43.687273Z"
    }
   },
   "outputs": [],
   "source": [
    "# 필요모듈 import \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.693997Z",
     "start_time": "2023-03-30T08:17:43.692302Z"
    }
   },
   "outputs": [],
   "source": [
    "# iris 데이터로드\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.697306Z",
     "start_time": "2023-03-30T08:17:43.695566Z"
    }
   },
   "outputs": [],
   "source": [
    "# 로딩 데이터 확인\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(iris['data'], columns=iris['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.700870Z",
     "start_time": "2023-03-30T08:17:43.699445Z"
    }
   },
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.703605Z",
     "start_time": "2023-03-30T08:17:43.701975Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "dtc = DecisionTreeClassifier(random_state=42, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.709489Z",
     "start_time": "2023-03-30T08:17:43.707708Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 분류\n",
    "dtc_pred = dtc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = dtc.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.712260Z",
     "start_time": "2023-03-30T08:17:43.710602Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델평가\n",
    "print(confusion_matrix(y_test, dtc_pred))\n",
    "print(classification_report(y_test, dtc_pred, target_names=iris['target_names']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 해석을 위한 시각화 방법\n",
    "#### feature importance\n",
    "트리 기반 모델은 트리를 분기하는 과정에서 어떤 변수가 모델을 학습하는데 중요한지 살펴볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.721885Z",
     "start_time": "2023-03-30T08:17:43.720414Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature importance 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.bar(iris['feature_names'], dtc.feature_importances_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(X_train, x='petal length (cm)', y='petal width (cm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.724423Z",
     "start_time": "2023-03-30T08:17:43.722760Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 시각화\n",
    "plot_tree(dtc,\n",
    "          feature_names=iris['feature_names'], \n",
    "          class_names=iris['target_names'], \n",
    "          filled=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가지치기 (pruning)\n",
    ">`Decision Tree`모델은 모든 **잎 노드의 불순도가 0이 되는 순간까지 모델을 성장**시키면서 크기를 키워나간다.  \n",
    "순수 노드로만 이루어진 트리 모델은 훈련 데이터를 100% 정확도로 맞출 수 있다.  \n",
    "이러한 특성 때문에 트리 모델은 **과적합에 취약**하다.  \n",
    "과적합 방지를 위해서는 **트리의 복잡도를 제어** 할 필요가 있다.\n",
    "\n",
    ">과적합 방지를 위한 모델링 파라메터  \n",
    ">> - **`max_depth`** : 트리의 최대 깊이  \n",
    "- `max_leaf_nodes` : 잎 노드의 최대개수  \n",
    "- `min_sample_leaf` : 잎 노드가 되기 위한 최소 샘플 갯수  \n",
    "- `min_sample_split` : 잎 노드가 분지 되기 위한 최소 샘플 갯수\n",
    "\n",
    "위의 iris 데이터는 3개의 클래스로 이루어진 데이터셋이지만 모델플로팅 결과 2뎁스의 노드에서 어느정도 데이터 구분이 되었습니다.  \n",
    "이를 기준으로 사후 가지치기를 진행 해 보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree regressor\n",
    "> `Decision Tree`모델은 알고리즘 특성으로 분류 및 예측 모델링에 모두 사용이 가능하다.  \n",
    "일반적으로 잎 노드에 속한 학습샘플의 값의 평균을 바탕으로 예측값을 결정한다.  \n",
    "회귀모델 평가 방법인 MSE를 각 노드에 속한 샘플에 적용하고 이를 최소화 시킨다.  \n",
    "\n",
    "<img src=\"./image/28.png\">\n",
    "<img src=\"./image/29.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree regressor 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.727123Z",
     "start_time": "2023-03-30T08:17:43.725585Z"
    }
   },
   "outputs": [],
   "source": [
    "# 보스턴 집값 데이터 로딩\n",
    "df = pd.read_csv('./data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.729344Z",
     "start_time": "2023-03-30T08:17:43.728103Z"
    }
   },
   "outputs": [],
   "source": [
    "# 타겟 데이터 분할\n",
    "y = df['y']\n",
    "X = df.drop('y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.731534Z",
     "start_time": "2023-03-30T08:17:43.730162Z"
    }
   },
   "outputs": [],
   "source": [
    "# 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.733831Z",
     "start_time": "2023-03-30T08:17:43.732394Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.736015Z",
     "start_time": "2023-03-30T08:17:43.734768Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "dtr = DecisionTreeRegressor(random_state=42, max_depth=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.738657Z",
     "start_time": "2023-03-30T08:17:43.736697Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "dtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.740834Z",
     "start_time": "2023-03-30T08:17:43.739512Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "dtr_pred = dtr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.743067Z",
     "start_time": "2023-03-30T08:17:43.741579Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 평가지표 출력\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, dtr_pred))\n",
    "print(mean_squared_error(y_test, dtr_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.8444833592340152\n",
    "11.588026315789474\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(X_train.columns, dtr.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.745394Z",
     "start_time": "2023-03-30T08:17:43.744066Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot_tree\n",
    "plot_tree(dtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-23T06:30:12.192008Z",
     "start_time": "2021-07-23T06:30:12.189331Z"
    }
   },
   "source": [
    "## Random Forest\n",
    ">**`Random forest`** 는 **`Decision Tree`** 모델의 **모형 결합(ensemble)방법론**  \n",
    "\n",
    "### ensemble(앙상블)\n",
    "> **복수의 예측 모형을 결합**하여 더 나은 성능의 예측을 하려는 시도이다.  \n",
    "단일 모형을 사용할 때 보다 **성능 분산이 감소**하고, 즉 **과적합을 방지**한다.  \n",
    "개별 모형이 성능이 안좋을 경우에는 결합 모형의 성능이 더 향상된다.  \n",
    "앙상블 방법론에는 **배깅**, **부스팅**이 있다.\n",
    "\n",
    "<img src=\"./image/30.gif\">\n",
    "\n",
    "#### bagging(배깅)\n",
    "> 개별 모델을 병렬로 구성하여 모델을 결합하는 방법론이다.  \n",
    "기존 학습데이터에서 **복원 추출**로 여러개의 sub sample 데이터셋을 만든 후 각 데이터셋을 병렬 구성 모델에 학습시켜 서로 다른 결과를 얻는다.  \n",
    "개별 모델의 결과값을 voting(투표법) 혹은 평균법을 사용하여 개별 모델 결과를 바탕으로 최종 추정치를 얻는다.  \n",
    "\n",
    "<img src=\"./image/31.png\">\n",
    "\n",
    "#### Random Forest Bootstrap Aggregating\n",
    "> **`Random forest`** 는 대표적인 배깅 방법론으로 weak model로 **`Decision Tree`** 를 사용한다.  \n",
    "배깅 사용 시 추가적으로 부트스트랩 방법론을 추가하여 모델 학습에 사용한다.  \n",
    "부트스트랩은 복원 추출 된 sub sample 데이터셋 생성 시 랜덤 샘플 및 feature를 선택하여 모델 학습에 사용한다.\n",
    "\n",
    "<img src=\"./image/32.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ramdom Forest 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.747650Z",
     "start_time": "2023-03-30T08:17:43.746352Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과적합 방지를 위한 모델링 파라메터  \n",
    "> - **n_estimators** : 사용 할 트리 모델 갯수  \n",
    "- **max_depth** : 트리의 최대 깊이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.750263Z",
     "start_time": "2023-03-30T08:17:43.748807Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "rfr = RandomForestRegressor(random_state=42, max_depth=11, n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.752550Z",
     "start_time": "2023-03-30T08:17:43.751015Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.754680Z",
     "start_time": "2023-03-30T08:17:43.753285Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "rfr_pred = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.756874Z",
     "start_time": "2023-03-30T08:17:43.755341Z"
    }
   },
   "outputs": [],
   "source": [
    "# 평가\n",
    "print(r2_score(y_test, rfr_pred))\n",
    "print(mean_squared_error(y_test, rfr_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dtr\n",
    "0.8523871394649085\n",
    "10.999091184097503\n",
    "rf\n",
    "0.8735309706700816\n",
    "3.0697880820727006\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Tree\n",
    "> 배깅과 부스팅의 차이점은 학습을 위해 사용하는 개별모델을 병렬/직렬로 구성함에 있다.  \n",
    "배깅의 경우 sub sample에 따라 개별 모델을 모두 학습시키고 결과를 투표 혹은 평균을 내어 예측한다면  \n",
    "부스팅은 **개별 모델의 학습을 순차적**으로 시키며 이전 개별 모델의 결과 중 **오분류 된 데이터 혹은 오차에 가중치 부여**  \n",
    "초기에는 동일 가중치를 갖지만 각 학습 과정을 거치며 복원 추출 시 가중치의 분포/이전 round의 오차를 고려  \n",
    "\n",
    ">> 해당모델에는 `Adaboost`, `GBM`, `Xgboost`, `lightGBM`, `catboost`가 있다.\n",
    "\n",
    "### bagging 과 boosting\n",
    "<img src=\"./image/33.png\">\n",
    "\n",
    "### Adaptive booting(Adaboost)\n",
    "> a -> f 순서로 학습이 진행 되고 있다. 각 학습 단계(round)에서 오분류 된 데이터에 가중치를 부여하고  \n",
    "다음 라운드에서 가중치가 부여 된 데이터를 잘 맞추기 위한 개별모델이 학습 된다.  \n",
    "최종 모델은 개별 모델의 결과가 합쳐져서 최종 모델링이 된다.\n",
    "\n",
    "<img src=\"./image/34.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient boost\n",
    "이전 round 모델의 데이터별 오류를 학습하는 모델을 사용하여 점진적으로 총 모델링 오차를 줄이는 부스팅 방법\n",
    "\n",
    "$$y = h_0(x) + error_0 $$\n",
    "$$error_0 = h_1(x) + error_1 $$\n",
    "$$error_1 = h_2(x) + error_2 $$\n",
    "$$\\vdots$$\n",
    "$$y = h_0(x) + h_1(x) + h_2(x) + \\cdots + small error $$\n",
    "\n",
    "<img src=\"./image/35.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xgboost\n",
    "> 머신러닝 알고리즘 대회인 kaggle, KDD cup등에서 우승을 한 팀들이 xgboost를 많이 활용한 것이 알려지면서 주목받음.  \n",
    "boosting 모델에서 오류를 학습하여 다음 round에 반영시키는 것은 gadient boosting과 큰 차이가 없음.  \n",
    "다만, 학습을 위한 비용함수에 규제화 식이 추가되어 모델이 과적합 되는 것을 방지함.  \n",
    "규제화를 통해 복잡한 모델에 패널티를 부여  \n",
    "\n",
    "$$obj^{(t)} = \\sum_1^{n} l(y_i, \\hat{y}_i^{(t)}) + \\sum_{i=1}^t \\Omega(f_i) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.759364Z",
     "start_time": "2023-03-30T08:17:43.757893Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 설치\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.767298Z",
     "start_time": "2023-03-30T08:17:43.765811Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 import\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.770049Z",
     "start_time": "2023-03-30T08:17:43.768531Z"
    }
   },
   "outputs": [],
   "source": [
    "# 보스턴 데이터 로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.772216Z",
     "start_time": "2023-03-30T08:17:43.770845Z"
    }
   },
   "outputs": [],
   "source": [
    "# 타겟 데이터 분할\n",
    "\n",
    "# 테스트셋 분할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.779565Z",
     "start_time": "2023-03-30T08:17:43.773091Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "xgbr = XGBRegressor(random_state=42)\n",
    "'''\n",
    "xbgoost 주요 파라메터\n",
    "\n",
    "모델 파라메터\n",
    "verbosity : round 출력결과 0=무음, 1=경고, 2=정보, 3=디버그\n",
    "n_jobs : 병렬쓰레드 구성, 로컬컴퓨터 코어 x 4 최대값\n",
    "gpu_id : GPU 연산 처리 디바이스 설정\n",
    "random_state : 랜덤시드\n",
    "missing : 결측치 처리 np.nan을 디폴트로 사용\n",
    "\n",
    "트리 파라메터\n",
    "max_depth : 트리모델 최대 깊이\n",
    "max_leaves : 트리모델 최대 잎 노드 갯수, 0=무제한 지정\n",
    "grow_policy : 트리확장 방법 0=노드와 가장 가까운 노드 분할, 1=손실함수가 최소가 되는 지점에서 분할\n",
    "gamma : 트리모델의 잎 노드 분할을 만드는 데 필요한 최소 손실 감소.\n",
    "min_child_weight : 관측치에 대한 최소 가중치 값\n",
    "subsample : 부트스트랩 샘플 비율\n",
    "colsample_bytree : 부트스트랩 컬럼 비율\n",
    "reg_alpha : L1, lasso, 0\n",
    "reg_lambda : L2, ridge, 1\n",
    "\n",
    "부스팅 파라메터\n",
    "n_estimators : 부스팅 트리 갯수, round 횟수와 같은 수\n",
    "learning_rate : round별 학습률\n",
    "booster: 부스팅 트리 모델 선택\n",
    "    gbtree\n",
    "    gblinear\n",
    "objective : 목적함수 \n",
    "    reg : squarederror\n",
    "    binary : logistic\n",
    "    multi : softmax\n",
    "    multi : softprob\n",
    "eval_metric : 모델평가함수, 목적함수에 따라 지정되어 있음\n",
    "    rmse: root mean square error\n",
    "    error: Binary classification error rate (0.5 threshold)\n",
    "    merror: Multiclass classification error rate\n",
    "early_stopping_rounds : 학습 손실값 변동 없을 시 학습 종료 라운드 횟수 설정\n",
    "callbacks : 학습 중 설정 값 전달 API\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.781886Z",
     "start_time": "2023-03-30T08:17:43.780510Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "xgbr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.784081Z",
     "start_time": "2023-03-30T08:17:43.782627Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "xgbr_pred = xgbr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.786525Z",
     "start_time": "2023-03-30T08:17:43.785173Z"
    }
   },
   "outputs": [],
   "source": [
    "# 평가지표 출력\n",
    "print(r2_score(y_test, xgbr_pred))\n",
    "print(mean_squared_error(y_test, xgbr_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dtr\n",
    "0.8523871394649085\n",
    "10.999091184097503\n",
    "rf\n",
    "0.8735309706700816\n",
    "3.0697880820727006\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.788987Z",
     "start_time": "2023-03-30T08:17:43.787599Z"
    }
   },
   "outputs": [],
   "source": [
    "# 변수 중요도 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 파라메터 서칭\n",
    "tree base 모델은 설정 가능한 파라메터의 조합에 따라 모델 예측력 차이가 큰 특징을 가지고 있습니다.  \n",
    "특히, Xgboost 모델의 경우 파라메터 설정에 따른 모델 예측력 차이가 굉장히 크기에 꼭 파라메터 서칭을 진행해주셔야 합니다.  \n",
    "간단한 문법을 통해 파라메터 서칭을 진행 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.791534Z",
     "start_time": "2023-03-30T08:17:43.789830Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train, y_train, random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.793527Z",
     "start_time": "2023-03-30T08:17:43.792281Z"
    }
   },
   "outputs": [],
   "source": [
    "# product 함수로 파라메터의 모든 조합 만들기\n",
    "best_score = 0\n",
    "best_param = 0\n",
    "\n",
    "depth = [9, 10, 11, 12, 13, 14]\n",
    "est = [100, 200, 300, 400]\n",
    "for param in list(product(depth, est)):\n",
    "    model = RandomForestRegressor(random_state=42, max_depth=param[0], n_estimators=param[1])\n",
    "    model.fit(X_train2, y_train2)\n",
    "    pred =model.predict(X_val)\n",
    "    r2 = r2_score(y_val, pred) # 결과값이 크면 클수록 좋은모델\n",
    "    if r2 > best_score:# 최대값 찾는 알고리즘\n",
    "        best_score = r2\n",
    "        best_param = param\n",
    "\n",
    "best_model = RandomForestRegressor(random_state=42, max_depth=best_param[0], n_estimators=best_param[1])\n",
    "best_model.fit(X_train, y_train)\n",
    "best_pred = best_model.predict(X_test)\n",
    "print(r2_score(y_test, best_pred))\n",
    "print(mean_squared_error(y_test, best_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.796127Z",
     "start_time": "2023-03-30T08:17:43.794569Z"
    }
   },
   "outputs": [],
   "source": [
    "# 위 파라메터 조합을 반복문으로 순환하며 파라메터 서칭\n",
    "best_score, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.801048Z",
     "start_time": "2023-03-30T08:17:43.799470Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 모델로 모델 다시 학습 및 평가\n",
    "best_model = RandomForestRegressor(random_state=42, max_depth=best_param[0], n_estimators=best_param[1])\n",
    "best_model.fit(X_train, y_train)\n",
    "best_pred = best_model.predict(X_test)\n",
    "print(r2_score(y_test, best_pred))\n",
    "print(mean_squared_error(y_test, best_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn GridSearchCV\n",
    "sklearn 패키지에는 위의 파라메터 서칭 과정을 간편하게 진행 할 수 있도록 GridSearchCV 방법론을 제공합니다.  \n",
    "기존 파라메터 서칭과 함께 cross validation 과정을 추가하여 데이터 분할에 강건한 모델을 선택할 수 있도록 제작 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.803536Z",
     "start_time": "2023-03-30T08:17:43.802156Z"
    }
   },
   "outputs": [],
   "source": [
    "# 그리드 서치 import\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그리드 서치에 사용할 파라메터 설정\n",
    "model = XGBRegressor(random_state=42)\n",
    "param = {\n",
    "    'max_depth' : [9, 10, 11, 12, 13],\n",
    "    'n_estimators' : [500, 700, 900, 1000],\n",
    "    'subsample' : [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree' : [0.7, 0.8, 0.9],\n",
    "#     'reg_alpha' : [1, 3, 5, 7, 9],\n",
    "#     'reg_lambda' : [1, 3, 5, 7, 9]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.806512Z",
     "start_time": "2023-03-30T08:17:43.804498Z"
    }
   },
   "outputs": [],
   "source": [
    "# 그리드 서치 실습\n",
    "grid = GridSearchCV(estimator=model,\n",
    "                    param_grid=param,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    verbose=2,\n",
    "                    cv=5)\n",
    "'''\n",
    "estimator : 모델 딕셔너리\n",
    "param_grid : 파라메터 딕셔너리\n",
    "scoring=None : 평가방법\n",
    "n_jobs=None : 학습에 사용할 컴퓨터 코어 갯수\n",
    "verbose=0 : 리포트 형식 0, 1, 2\n",
    "\n",
    "scoring 참고\n",
    "https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.808587Z",
     "start_time": "2023-03-30T08:17:43.807186Z"
    }
   },
   "outputs": [],
   "source": [
    "# grid 학습\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-30T08:17:43.811030Z",
     "start_time": "2023-03-30T08:17:43.809500Z"
    }
   },
   "outputs": [],
   "source": [
    "# 최적 모델 및 파라메터 확인\n",
    "best_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(r2_score(y_test, best_pred))\n",
    "print(mean_squared_error(y_test, best_pred, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 디스이즈 컴퍼티션!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kospi = pd.read_csv('./data/kospi.csv', encoding='cp949')\n",
    "kospi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_size=0.3, random_state=42"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
